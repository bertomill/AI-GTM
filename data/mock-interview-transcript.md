# KPMG AI Strategy Mock Interview Transcript

## Stakeholders Present
- **Sarah Chen** - Tax Practice Lead
- **Marcus Rodriguez** - Head of Technology  
- **Dr. Jennifer Walsh** - Product Manager
- **Robert Mill** - Presenting (AI Solutions & Go-to-Market Consultant)

---

## Opening & Three Strategic Questions

**Interviewer (Sarah Chen):** Robert, thank you for joining us today. We're excited to hear your strategic approach to scaling AI adoption across our Tax functions. Before we begin your presentation, you have the opportunity to ask up to three questions to help frame your recommendations. What would you like to know?

**Robert:** Thank you, Sarah. I appreciate this opportunity to collaborate on such a critical initiative. I have three strategic questions that will help me tailor my recommendations:

1. **Stakeholder Priorities:** What are each of your primary success metrics for this AI rollout? Sarah, what does success look like from a Tax practice perspective? Marcus, from a technology standpoint? And Jennifer, from a product development angle?

2. **Resource Reality:** What level of time investment are we realistically able to ask from our tax professionals during the rollout phases? Are we talking about 2-3 hours per week for training, or do we need to be more conservative?

3. **Organizational Dynamics:** Based on your experience with previous technology implementations, what has been our biggest barrier - is it typically resistance from senior professionals, middle management skepticism, or something else entirely?

**Interviewer (Sarah Chen):** Excellent questions, Robert. Let me address the priorities first. From my perspective, success means maintaining our client service quality while achieving measurable efficiency gains. I need to see that this doesn't disrupt our current delivery timelines.

**Interviewer (Marcus Rodriguez):** From the technology side, I'm focused on adoption rates and support scalability. If we can't support 200+ users effectively, the whole initiative fails. I also need security and integration to be seamless.

**Interviewer (Dr. Jennifer Walsh):** Product-wise, I want to see actual usage metrics - not just initial adoption, but sustained engagement. Are people using it daily after month three? And are we seeing the productivity improvements we projected?

**Interviewer (Sarah Chen):** For resources, realistically we can commit to 3-4 hours per person in the first month, then maybe 1-2 hours monthly for ongoing development. And historically, our biggest barrier has been middle management - partners are either all-in or all-out, but managers worry about looking incompetent or being replaced.

**Robert:** Perfect. That context is incredibly valuable and confirms some of my analysis. Let me present a strategy that directly addresses these priorities and constraints.

## Strategic Presentation

**Robert:** [SAMPLE OPENING] 

"Thank you for that context. Based on our pilot results and your insights, I see a clear path forward that builds on what's working while systematically addressing our adoption barriers.

Here's what we know: Our pilot group achieved 40% efficiency gains in contract review, but adoption remains limited to just 12 users. The technology works - our challenge is organizational, not technical.

I'm proposing a 4-phase approach that treats this as a change management initiative first, and a technology rollout second. This strategy addresses Sarah's client service concerns, Marcus's scalability requirements, and Jennifer's engagement metrics.

The core insight driving my approach is this: People don't resist AI - they resist uncertainty. So our job is to systematically reduce uncertainty through champions, training, and measurable wins."

**Interviewer (Sarah Chen):** Robert, that opening is excellent. You've clearly listened to our concerns and I appreciate how you've positioned this as change management rather than just technology deployment. That resonates with my experience - we've had tools that technically worked but never got adopted.

I'm particularly interested in your 4-phase approach. Can you walk us through what that looks like practically? And when you mention "champions" - how do we identify and leverage them without creating additional workload for our best performers?

**Interviewer (Dr. Jennifer Walsh):** I love the framing around uncertainty reduction. That's exactly what our user research showed - people weren't afraid of AI, they were afraid of looking incompetent or making mistakes. How does your approach specifically address that psychological barrier?

**Interviewer (Marcus Rodriguez):** And from a practical standpoint, Robert, what's the timeline we're looking at for each phase? I need to plan support resources and integration work accordingly.

**Robert:** Thank you. Let me outline the four phases, with specific timelines and deliverables for each.

**Phase 1: Champion Network - 30 Days**
We start by identifying 8-10 champions across different tax specialties - not necessarily our busiest people, but those who are naturally curious and influential among their peers. These champions get intensive training and become our internal evangelists.

**Phase 2: Department Pilots - 60 Days**  
Each champion leads a small pilot within their specialty area - maybe 3-4 additional team members. This creates multiple success stories across different tax functions rather than one isolated group.

**Phase 3: Full Scale Deployment - 90 Days**
Based on pilot learnings, we roll out to the broader organization with refined training materials, peer mentoring, and clear success metrics.

**Phase 4: Optimization & Expansion - 120+ Days**
We measure, iterate, and scale to additional AI tools based on adoption success.

**Interviewer (Marcus Rodriguez):** That timeline seems aggressive, Robert. In Phase 2, when you have multiple pilots running simultaneously, what's the support structure? Are we talking about dedicated technical resources for each pilot group?

**Interviewer (Dr. Jennifer Walsh):** And how do you ensure the champions don't burn out? In my experience, we often overload our best people with additional responsibilities without considering their existing workload.

**Interviewer (Sarah Chen):** I'm also curious about the metrics. How do we measure success at each phase, and what happens if a pilot group struggles? Do we pause the whole initiative or adapt?

**Robert:** Excellent questions - let me get specific about Phase 1 implementation since that's our foundation.

## Phase 1: Champion Network - Detailed Implementation

**Champion Selection Process:**
We identify champions through a three-pronged approach:
1. **Performance Analytics** - Who from the pilot group had highest engagement rates and best results?
2. **Peer Nominations** - We survey the tax teams: "Who do you go to when you need help with new processes?"
3. **Manager Recommendations** - Each department head identifies 2-3 natural influencers

**Champion Outreach Strategy:**
Initial contact is a personalized email from Sarah positioning this as a leadership development opportunity:

*"Hi [Name], Based on your success with our AI pilot and your reputation as someone colleagues turn to for guidance, I'd like to invite you to join our AI Champion Network. This is a 4-week commitment that will position you as a thought leader in our AI transformation while giving you early access to advanced tools and direct input on our rollout strategy. Are you interested in a brief conversation about this opportunity?"*

**Champion Collaboration Platform:**
We set up a dedicated Slack channel called #ai-champions with:
- Weekly office hours with the AI team
- Shared resource library in SharePoint
- Monthly video calls for strategy alignment
- Direct line to escalate technical issues to Marcus's team

**Champion Time Investment:**
- Week 1: 3-hour intensive training session (recorded for reference)
- Weeks 2-4: 1 hour weekly check-ins via Slack and brief video calls
- Ongoing: 30 minutes per week sharing updates and mentoring 2-3 peers
- Recognition: "AI Champion" designation on internal directory and LinkedIn

**Champion Responsibilities:**
1. Test new AI features and provide feedback
2. Document common use cases and success stories  
3. Mentor 2-3 colleagues in their department
4. Escalate technical issues and user feedback
5. Co-create training materials based on real experience

**Interviewer (Marcus Rodriguez):** That's much more concrete, Robert. I like the Slack approach - it's infrastructure we already have. But what about the technical support? When a champion runs into an AI tool issue, what's the escalation path?

**Interviewer (Dr. Jennifer Walsh):** And how do we prevent champion burnout? What's the recognition or incentive structure for taking on this additional responsibility?

---

*Great detail! They can see you've thought through the practical implementation. How do you address their follow-up concerns about support and incentives?*